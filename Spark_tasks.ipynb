{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niknik26/spark_task/blob/main/Spark_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMuH9zmJkosZ"
      },
      "source": [
        "Самостоятельное задание  №1. Погода"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od8DgIUaNvSG",
        "outputId": "6bab8a82-d771-4c7e-a678-ec03f48b926c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=7b8bf4b646d11ee3aee7f9d4229209da5cb7ba313d6e3307804b4f999af96849\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3QwiTEmkVXc",
        "outputId": "f556442d-5c80-4e4c-8159-8ff79cea26f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----------+-------------+----------+\n",
            "|station_id|date|temperature|precipitation|wind_speed|\n",
            "+----------+----+-----------+-------------+----------+\n",
            "+----------+----+-----------+-------------+----------+\n",
            "\n",
            "НЕТ пропущенных значений\n",
            "+----------+------------------+\n",
            "|      date|       temperature|\n",
            "+----------+------------------+\n",
            "|2021-08-20|39.982828249354846|\n",
            "|2023-12-02| 39.96797489293784|\n",
            "|2022-03-28|  39.8246894248997|\n",
            "|2019-02-11| 39.76737697836647|\n",
            "|2020-06-10| 39.69147838355929|\n",
            "+----------+------------------+\n",
            "\n",
            "+----------+------------------+\n",
            "|station_id|sum(precipitation)|\n",
            "+----------+------------------+\n",
            "| station_5| 642.9302626767898|\n",
            "+----------+------------------+\n",
            "\n",
            "+-----+------------------+\n",
            "|month|avg(precipitation)|\n",
            "+-----+------------------+\n",
            "|   12| 30.01257487211919|\n",
            "|    1|25.788403692179923|\n",
            "|    6| 25.48369925567644|\n",
            "|    3|25.992138694271425|\n",
            "|    5|24.771692387500345|\n",
            "|    9|26.306950943105992|\n",
            "|    4| 24.20807094379405|\n",
            "|    8|27.466365894503493|\n",
            "|    7|24.935742740277306|\n",
            "|   10|26.588845479108233|\n",
            "|   11| 23.76319068746788|\n",
            "|    2|24.782723612839384|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Самостоятельное задание  №1. Погода\n",
        "# Создание SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, mean, max, avg, month\n",
        "import pandas as pd\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Read weather\").getOrCreate()\n",
        "\n",
        "# Чтение CSV-файла\n",
        "#df = spark.read.csv(\"/content/sample_data/weather_data_n.csv\", header=True, inferSchema=True)\n",
        "df = spark.read.csv(\"/content/weather_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Cр. значения по метеостанциям\n",
        "# mean_t = df.select(mean(col('temperature'))).collect()[0][0]\n",
        "# df_t = df.na.fill({\"temperature\": mean_t}).show()\n",
        "# mean_p = df.select(mean(col('precipitation'))).collect()[0][0]\n",
        "# df_p = df.na.fill({\"precipitation\": mean_p}).show()\n",
        "# mean_w = df.select(mean(col('wind_speed'))).collect()[0][0]\n",
        "# df_w = df.na.fill({\"wind_speed\": mean_w}).show()\n",
        "\n",
        "# Вывод строк с пустыми данными, если есть.\n",
        "df_nulls = df.filter(col(\"temperature\").isNull()|col(\"precipitation\").isNull()|col(\"wind_speed\").isNull())\n",
        "\n",
        "# Подсчет количества таких строк\n",
        "null_count = df_nulls.count()\n",
        "df_nulls.show()\n",
        "if null_count > 0:\n",
        "    print(f\"Отсутствуют значения погоды. Всего строк {null_count}\")\n",
        "else:\n",
        "    print(\"НЕТ пропущенных значений\")\n",
        "\n",
        "# Cредние значения по метеостанциям\n",
        "mean_all = {\n",
        "    \"temperature\": df.select(mean(col(\"temperature\"))).collect()[0][0],\n",
        "    \"precipitation\": df.select(mean(col(\"precipitation\"))).collect()[0][0],\n",
        "    \"wind_speed\": df.select(mean(col(\"wind_speed\"))).collect()[0][0]\n",
        "}\n",
        "# Заполнить отсутствующие значения\n",
        "df_filled = df.na.fill(mean_all)\n",
        "\n",
        "# 3. Анализ данных:\n",
        "# 3.1.Найдите топ-5 самых жарких дней за все время наблюдений.\n",
        "# df.select(max(\"temperature\")).show()\n",
        "df.select(col(\"date\"),col(\"temperature\")).sort(col(\"temperature\").desc(),col(\"date\")).limit(5).show()\n",
        "\n",
        "# 3.2.Найдите метеостанцию с наибольшим количеством осадков за последний год.\n",
        "dfGroup = df.filter(col(\"date\") >= \"2023-01-01\").select(col(\"station_id\"),col(\"precipitation\")).groupBy(\"station_id\").sum().sort(col(\"sum(precipitation)\").desc())\n",
        "dfGroup.limit(1).show()\n",
        "\n",
        "# 3.3.Подсчитайте среднюю температуру по месяцам за все время наблюдений.\n",
        "# Месяц даты извлекаем\n",
        "df = df.withColumn(\"month\", month(col(\"date\")))\n",
        "\n",
        "df_grouped = df.groupBy(\"month\").mean(\"precipitation\").show()\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRtoh567Vtws"
      },
      "source": [
        "Самостоятельное задание  №2. Книги и авторы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk_XlovlV9-j",
        "outputId": "25466f2a-d4ff-4a20-d507-5d65d044af29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+----------+---------+-------+-------+-----------+-----+------------+\n",
            "|author_id|     name|birth_date|  country|book_id|  title|      genre|price|publish_date|\n",
            "+---------+---------+----------+---------+-------+-------+-----------+-----+------------+\n",
            "|        2| Author_2|1965-12-31|   Canada|      1| Book_1|    Mystery|73.57|  1980-12-31|\n",
            "|        1| Author_1|1960-12-31|    India|      2| Book_2|Non-Fiction| 41.1|  1982-12-31|\n",
            "|       10|Author_10|2005-12-31|    India|      3| Book_3|    Fiction|10.63|  1984-12-31|\n",
            "|        9| Author_9|2000-12-31|Australia|      4| Book_4|Non-Fiction|46.31|  1986-12-31|\n",
            "|        7| Author_7|1990-12-31|      USA|      5| Book_5|    Science|31.13|  1988-12-31|\n",
            "|        4| Author_4|1975-12-31|       UK|      6| Book_6|Non-Fiction| 83.7|  1990-12-31|\n",
            "|        6| Author_6|1985-12-31|      USA|      7| Book_7|Non-Fiction|40.36|  1992-12-31|\n",
            "|        2| Author_2|1965-12-31|   Canada|      8| Book_8|Non-Fiction|84.48|  1994-12-31|\n",
            "|        7| Author_7|1990-12-31|      USA|      9| Book_9|    Fantasy|10.05|  1996-12-31|\n",
            "|        2| Author_2|1965-12-31|   Canada|     10|Book_10|    Science| 37.7|  1998-12-31|\n",
            "|       10|Author_10|2005-12-31|    India|     11|Book_11|Non-Fiction| 31.7|  2000-12-31|\n",
            "|        8| Author_8|1995-12-31|Australia|     12|Book_12|Non-Fiction|31.02|  2002-12-31|\n",
            "|        8| Author_8|1995-12-31|Australia|     13|Book_13|Non-Fiction|16.14|  2004-12-31|\n",
            "|        1| Author_1|1960-12-31|    India|     14|Book_14|    Fiction|26.84|  2006-12-31|\n",
            "|        8| Author_8|1995-12-31|Australia|     15|Book_15|    Fantasy| 60.0|  2008-12-31|\n",
            "|        2| Author_2|1965-12-31|   Canada|     16|Book_16|    Fiction|36.22|  2010-12-31|\n",
            "|        6| Author_6|1985-12-31|      USA|     17|Book_17|    Fantasy|47.57|  2012-12-31|\n",
            "|        1| Author_1|1960-12-31|    India|     18|Book_18|Non-Fiction|43.92|  2014-12-31|\n",
            "|        5| Author_5|1980-12-31|      USA|     19|Book_19|    Science|88.83|  2016-12-31|\n",
            "|        7| Author_7|1990-12-31|      USA|     20|Book_20|    Mystery|91.48|  2018-12-31|\n",
            "+---------+---------+----------+---------+-------+-------+-----------+-----+------------+\n",
            "\n",
            "+---------+--------+-------------+\n",
            "|author_id|    name|total_revenue|\n",
            "+---------+--------+-------------+\n",
            "|        2|Author_2|       231.97|\n",
            "|        7|Author_7|       132.66|\n",
            "|        1|Author_1|       111.86|\n",
            "|        8|Author_8|       107.16|\n",
            "|        5|Author_5|        88.83|\n",
            "+---------+--------+-------------+\n",
            "\n",
            "+-----------+-----+\n",
            "|      genre|count|\n",
            "+-----------+-----+\n",
            "|Non-Fiction|    9|\n",
            "|    Science|    3|\n",
            "|    Fiction|    3|\n",
            "|    Fantasy|    3|\n",
            "|    Mystery|    2|\n",
            "+-----------+-----+\n",
            "\n",
            "+---------+---------+-----------------+\n",
            "|author_id|     name|    average_price|\n",
            "+---------+---------+-----------------+\n",
            "|        5| Author_5|            88.83|\n",
            "|        4| Author_4|             83.7|\n",
            "|        2| Author_2|          57.9925|\n",
            "|        9| Author_9|            46.31|\n",
            "|        7| Author_7|            44.22|\n",
            "|        6| Author_6|           43.965|\n",
            "|        1| Author_1|37.28666666666667|\n",
            "|        8| Author_8|            35.72|\n",
            "|       10|Author_10|           21.165|\n",
            "+---------+---------+-----------------+\n",
            "\n",
            "+---------+-------+-------+-----------+-----+------------+---------+----------+---------+\n",
            "|author_id|book_id|  title|      genre|price|publish_date|     name|birth_date|  country|\n",
            "+---------+-------+-------+-----------+-----+------------+---------+----------+---------+\n",
            "|        7|     20|Book_20|    Mystery|91.48|  2018-12-31| Author_7|1990-12-31|      USA|\n",
            "|        5|     19|Book_19|    Science|88.83|  2016-12-31| Author_5|1980-12-31|      USA|\n",
            "|        8|     15|Book_15|    Fantasy| 60.0|  2008-12-31| Author_8|1995-12-31|Australia|\n",
            "|        6|     17|Book_17|    Fantasy|47.57|  2012-12-31| Author_6|1985-12-31|      USA|\n",
            "|        1|     18|Book_18|Non-Fiction|43.92|  2014-12-31| Author_1|1960-12-31|    India|\n",
            "|        2|     16|Book_16|    Fiction|36.22|  2010-12-31| Author_2|1965-12-31|   Canada|\n",
            "|       10|     11|Book_11|Non-Fiction| 31.7|  2000-12-31|Author_10|2005-12-31|    India|\n",
            "|        8|     12|Book_12|Non-Fiction|31.02|  2002-12-31| Author_8|1995-12-31|Australia|\n",
            "|        1|     14|Book_14|    Fiction|26.84|  2006-12-31| Author_1|1960-12-31|    India|\n",
            "|        8|     13|Book_13|Non-Fiction|16.14|  2004-12-31| Author_8|1995-12-31|Australia|\n",
            "+---------+-------+-------+-----------+-----+------------+---------+----------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Самостоятельное задание  №2. Книги и авторы\n",
        "# Создание SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, mean, max, avg, month, year, when\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"BOOKS\").getOrCreate()\n",
        "\n",
        "# Чтение CSV-файла\n",
        "df_books = spark.read.csv(\"/content/books.csv\", header=True, inferSchema=True)\n",
        "df_authors = spark.read.csv(\"/content/authors.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Преобразование publish_date и birth_date в формат даты.\n",
        "df_b = df_books.withColumn(\"publish_date\", to_date(col(\"publish_date\"), \"yyyy-MM-dd\"))\n",
        "df_a = df_authors.withColumn(\"birth_date\", to_date(col(\"birth_date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "# 4.1 топ-5 авторов, книги которых принесли наибольшую выручку\n",
        "# Объедините таблицы books и authors по author_id.\n",
        "inner_join_df = df_a.join(df_b, df_a.author_id == df_b.author_id, \"inner\").drop(df_b[\"author_id\"])\n",
        "inner_join_df.show()\n",
        "\n",
        "# Группировка по author_id из df_a и вычисление суммы price из df_b, переименование столбца(sum(price) в total_revenue)упорядочение по убыванию и выбор топ 5 авторов\n",
        "aa = inner_join_df.groupBy(\"author_id\").sum(\"price\")\n",
        "aa = aa.withColumnRenamed(\"sum(price)\", \"total_revenue\")\n",
        "aa1 = aa.sort(col(\"total_revenue\").desc()).limit(5)\n",
        "\n",
        "# Присоединение столбца \"name\" и удаление лишних столбцов\n",
        "inner_aa= aa1.join(df_a, aa1.author_id == df_a.author_id, \"left_outer\").drop(aa1[\"author_id\"]).select(\"author_id\", \"name\", \"total_revenue\")\n",
        "inner_aa.show()\n",
        "\n",
        "# 4.2 количество книг в каждом жанре.\n",
        "sum_g=inner_join_df.groupBy(\"genre\").count().sort(col(\"count\").desc())\n",
        "sum_g.show()\n",
        "\n",
        "# 4.3 средняя цену книг по каждому автору   удалил .desc()\n",
        "avg_price = inner_join_df.groupBy(\"author_id\").mean(\"price\").sort(col(\"avg(price)\"))\n",
        "avg_price = avg_price.withColumnRenamed(\"avg(price)\", \"average_price\")\n",
        "\n",
        "# Присоединение столбца \"name\" и удаление лишних столбцов\n",
        "avg_price = avg_price.join(df_a, avg_price.author_id == df_a.author_id, \"left_outer\").drop(avg_price[\"author_id\"]).select(\"author_id\", \"name\", \"average_price\")\n",
        "avg_price = avg_price.sort(col(\"average_price\").desc())\n",
        "avg_price.show()\n",
        "\n",
        "# 4.4 Книги, опубликованные после 2000 года, и отсортируйте их по цене\n",
        "year = inner_join_df.filter(year(\"publish_date\") >= 2000)\n",
        "book_year = year.select(\"author_id\", \"book_id\", \"title\", \"genre\", \"price\", \"publish_date\", \"name\", \"birth_date\", \"country\").sort(col(\"price\").desc())\n",
        "book_year.show()\n",
        "\n",
        "spark.stop()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Самостоятельное задание  №3. Фильмы и актеры. С использованием временных таблиц и SQL"
      ],
      "metadata": {
        "id": "xISfZPHYou_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Самостоятельное задание  №3. Фильмы и актеры. С использованием временных таблиц и SQL\n",
        "# Создание SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, mean, max, avg, month, year, when\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Movies\").getOrCreate()\n",
        "\n",
        "# Чтение CSV-файла\n",
        "df_movie_actors = spark.read.csv(\"/content/movie_actors.csv\", header=True, inferSchema=True)\n",
        "df_movie = spark.read.csv(\"/content/movies.csv\", header=True, inferSchema=True)\n",
        "df_actors = spark.read.csv(\"/content/actors.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Регистрация DataFrame как временные таблицы\n",
        "df_movie_actors.createOrReplaceTempView(\"movie_actorsT\")\n",
        "df_movie.createOrReplaceTempView(\"movieT\")\n",
        "df_actors.createOrReplaceTempView(\"actorsT\")\n",
        "\n",
        "# 4.1 топ-5 жанров по количеству фильмов.\n",
        "genre_name = spark.sql(\"\"\"\n",
        "SELECT genre, COUNT(movie_id) AS num_movies\n",
        "FROM movieT\n",
        "GROUP BY genre\n",
        "ORDER BY num_movies DESC\n",
        "\"\"\")\n",
        "genre_name.show()\n",
        "\n",
        "# 4.2 актер с наибольшим количеством фильмов.\n",
        "# Выполнение JOIN-запроса три таблицы с использованием SQL, без дублирующих связывающих столбцов\n",
        "join_df = spark.sql(\"\"\"\n",
        "SELECT mt.movie_id, title, genre, release_date, budget, at.actor_id, name, birth_date, country\n",
        "FROM movieT mt\n",
        "JOIN movie_actorsT mat\n",
        "ON mt.movie_id = mat.movie_id\n",
        "JOIN actorsT at\n",
        "ON mat.actor_id = at.actor_id\n",
        "\"\"\")\n",
        "join_df.show()\n",
        "# Регистрация join_df как временной таблицы\n",
        "join_df.createOrReplaceTempView(\"join_df_temp\")\n",
        "\n",
        "act_max = spark.sql(\"\"\"\n",
        "SELECT name, COUNT (movie_id) AS num_movies\n",
        "FROM join_df_temp\n",
        "GROUP BY name\n",
        "ORDER BY num_movies DESC\n",
        "limit(1)\n",
        "\"\"\")\n",
        "act_max.show()\n",
        "\n",
        "# 4.3 Cредний бюджет фильмов по жанрам\n",
        "bud = spark.sql(\"\"\"\n",
        "SELECT genre, AVG(budget) AS avg_budget\n",
        "FROM movieT\n",
        "GROUP BY genre\n",
        "\"\"\")\n",
        "bud.show()\n",
        "\n",
        "# 4.4 Фильмы, в которых снялось более одного актера из одной страны.\n",
        "more1act = spark.sql(\"\"\"\n",
        "SELECT title, country, COUNT(DISTINCT actor_id) AS num_actors\n",
        "FROM join_df_temp\n",
        "GROUP BY title, country\n",
        "HAVING num_actors > 1\n",
        "\"\"\")\n",
        "more1act.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg6qqRJ1p6Qo",
        "outputId": "336d12b5-b690-4d47-88d7-27e00119c909"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+\n",
            "| genre|num_movies|\n",
            "+------+----------+\n",
            "| Drama|         6|\n",
            "|Action|         6|\n",
            "|Comedy|         4|\n",
            "|Horror|         2|\n",
            "|Sci-Fi|         2|\n",
            "+------+----------+\n",
            "\n",
            "+--------+-------+------+------------+-------------+--------+--------+----------+---------+\n",
            "|movie_id|  title| genre|release_date|       budget|actor_id|    name|birth_date|  country|\n",
            "+--------+-------+------+------------+-------------+--------+--------+----------+---------+\n",
            "|       1|Movie_1|Horror|  2000-12-31|8.660058311E7|      13|Actor_13|1984-12-31|       UK|\n",
            "|       1|Movie_1|Horror|  2000-12-31|8.660058311E7|      16|Actor_16|1990-12-31|    India|\n",
            "|       1|Movie_1|Horror|  2000-12-31|8.660058311E7|      25|Actor_25|2008-12-31|    India|\n",
            "|       1|Movie_1|Horror|  2000-12-31|8.660058311E7|       8| Actor_8|1974-12-31|Australia|\n",
            "|       1|Movie_1|Horror|  2000-12-31|8.660058311E7|      25|Actor_25|2008-12-31|    India|\n",
            "|       2|Movie_2|Comedy|  2001-12-31|1.274740083E7|      17|Actor_17|1992-12-31|      USA|\n",
            "|       2|Movie_2|Comedy|  2001-12-31|1.274740083E7|      17|Actor_17|1992-12-31|      USA|\n",
            "|       2|Movie_2|Comedy|  2001-12-31|1.274740083E7|      24|Actor_24|2006-12-31|Australia|\n",
            "|       3|Movie_3|Action|  2002-12-31| 1.80157747E7|      17|Actor_17|1992-12-31|      USA|\n",
            "|       3|Movie_3|Action|  2002-12-31| 1.80157747E7|      13|Actor_13|1984-12-31|       UK|\n",
            "|       3|Movie_3|Action|  2002-12-31| 1.80157747E7|       6| Actor_6|1970-12-31|      USA|\n",
            "|       4|Movie_4| Drama|  2003-12-31|4.817612061E7|      18|Actor_18|1994-12-31|       UK|\n",
            "|       4|Movie_4| Drama|  2003-12-31|4.817612061E7|      21|Actor_21|2000-12-31|   Canada|\n",
            "|       6|Movie_6|Action|  2005-12-31|1.476121831E7|      24|Actor_24|2006-12-31|Australia|\n",
            "|       6|Movie_6|Action|  2005-12-31|1.476121831E7|      16|Actor_16|1990-12-31|    India|\n",
            "|       7|Movie_7| Drama|  2006-12-31|4.456703643E7|      16|Actor_16|1990-12-31|    India|\n",
            "|       7|Movie_7| Drama|  2006-12-31|4.456703643E7|      17|Actor_17|1992-12-31|      USA|\n",
            "|       7|Movie_7| Drama|  2006-12-31|4.456703643E7|       9| Actor_9|1976-12-31|      USA|\n",
            "|       7|Movie_7| Drama|  2006-12-31|4.456703643E7|      25|Actor_25|2008-12-31|    India|\n",
            "|       7|Movie_7| Drama|  2006-12-31|4.456703643E7|       1| Actor_1|1960-12-31|   Canada|\n",
            "+--------+-------+------+------------+-------------+--------+--------+----------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------+----------+\n",
            "|    name|num_movies|\n",
            "+--------+----------+\n",
            "|Actor_17|         5|\n",
            "+--------+----------+\n",
            "\n",
            "+------+--------------------+\n",
            "| genre|          avg_budget|\n",
            "+------+--------------------+\n",
            "| Drama| 6.076021856166667E7|\n",
            "|Horror|      8.7281876775E7|\n",
            "|Comedy|     5.20709662225E7|\n",
            "|Action|2.7492742561666667E7|\n",
            "|Sci-Fi|       7.809715175E7|\n",
            "+------+--------------------+\n",
            "\n",
            "+--------+---------+----------+\n",
            "|   title|  country|num_actors|\n",
            "+--------+---------+----------+\n",
            "| Movie_7|    India|         2|\n",
            "| Movie_3|      USA|         2|\n",
            "|Movie_10|       UK|         2|\n",
            "|Movie_15|    India|         2|\n",
            "|Movie_18|Australia|         2|\n",
            "| Movie_1|    India|         2|\n",
            "| Movie_7|      USA|         2|\n",
            "|Movie_10|      USA|         2|\n",
            "+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgw3EQxXG4zREMhBuNdkwr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}